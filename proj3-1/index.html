<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>
  body {
    background-color: white;
    padding: 100px;
    width: 1000px;
    margin: auto;
    text-align: left;
    font-weight: 300;
    font-family: 'Open Sans', sans-serif;
    color: #121212;
  }
  h1, h2, h3, h4 {
    font-family: 'Source Sans Pro', sans-serif;
  }
  kbd {
    color: #121212;
  }
</style>
<title>CS 184 Path Tracer</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link href="https://fonts.googleapis.com/css?family=Open+Sans|Source+Sans+Pro" rel="stylesheet">

<script>
  MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']]
    }
  };
</script>
<script id="MathJax-script" async
  src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js">
</script>

</head>


<body>

<h1 align="middle">CS 184: Computer Graphics and Imaging, Spring 2023</h1>
<h1 align="middle">Project 3-1: Path Tracer</h1>
<h2 align="middle">Patrick Zhu and Tymon Thi</h2>

<!-- Add Website URL -->
<h2 align="middle">Website URL: <a href="https://patrickrz.github.io/proj-webpage-template/proj3-1/index.html">link</a></h2>

<br><br>


<div align="center">
  <table style="width=100%">
      <tr>
          <td align="middle">
          <img src="images/part5/spheres_adaptive_sampling.png" width="480px" />
      </tr>
  </table>
</div>




<h2 align="middle">Overview</h2>
<p>
    Project 3-1 follows the steps of building a path tracer. We begin by generating camera rays that cross from camera
    to world space and also calibrate how our algorithms generates pixel samples. Then, we implement ray-triangle
    and ray-circle intersection in order to be able to render these types of shapes. Part 2 follows the construction of
    a Bounding Volume Hierarchy, a data structure that partitions parts of a mesh and facilitates a speed-up of our
    intersection algorithms. With this, we move on to direct and global illumination in Parts 3 and 4, essentially
    sampling pixels and estimating their radiance via Monte Carlo estimation. Global illumination in particular is a
    recursive algorithm that samples more than one bounce of light to achieve greater "visual richness." Lastly,
    Adaptive Sampling is implemented in Part 5. This method speeds up our algorithms by detecting when sampling of a
    pixel has converged, utilizing statistics and confidence intervals. This project has been challenging but rewarding.
    Rendering images was not a trivial feat and paying attention to even the smallest details made the difference
    between a perfect and a "bug art" render.
</p>
<br>

<h2 align="middle">Part 1: Ray Generation and Scene Intersection (20 Points)</h2>
<!-- Walk through the ray generation and primitive intersection parts of the rendering pipeline.
Explain the triangle intersection algorithm you implemented in your own words.
Show images with normal shading for a few small .dae files. -->

<h3>
  Walk through the ray generation and primitive intersection parts of the rendering pipeline.
</h3>
<p>
  The first step of the ray generation process is generating a camera ray.
  This is done by taking in a normalized point in image space, translating it
  into camera sensor space and using it to generate a ray in camera space and
  then finally transforming this ray back into world space. Next, we sample
  several random, generated rays, calculating a Monte Carlo estimate for the
  "scene radiance" in each pixel. Then, ray-triangle intersection was
  implemented using the Moller Trumbore algorithm and ray-sphere intersection
  was implemented with the provided quadratic equation.
</p>
<br>

<h3>
  Explain the triangle intersection algorithm you implemented in your own words.
</h3>
<p>
  As mentioned above, we implemented the Moller Trumbore algorithm in order
  to enable triangle intersection. Essentially, we perform matrix
  multiplication as outlined in the slide linked <a
  href="https://cs184.eecs.berkeley.edu/sp23/lecture/9-22/intro-to-ray-tracing-and-acceler">here</a>.
  The 3D vector that is the result of the matrix multiplication offers a
  <code>t</code> value and two barycentric coordinates, both of which can be
  used to determine if an intersection truly occurs. Additionally, the
  barycentric coordinates are used to calculate the surface normal at the
  intersection point on the triangle.
</p>
<br>

<h3>
  Show images with normal shading for a few small .dae files.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/part1/CBempty_writeup.png" align="middle" width="400px"/>
        <figcaption>CBempty.dae</figcaption>
      </td>
      <td>
        <img src="images/part1/CBspheres_writeup.png" align="middle" width="400px"/>
        <figcaption>CBspheres_lambertian.dae</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/part1/cube_writeup.png" align="middle" width="400px"/>
        <figcaption>cube.dae</figcaption>
      </td>
      <td>
        <img src="images/part1/gems_writeup.png" align="middle" width="400px"/>
        <figcaption>gems.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>


<h2 align="middle">Part 2: Bounding Volume Hierarchy (20 Points)</h2>
<!-- Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis. -->

<h3>
  Walk through your BVH construction algorithm. Explain the heuristic you chose for picking the splitting point.
</h3>
<p>
  To implement <code>BHVAccel::construct_bvh()</code>, we first create a new
  <code>BBox</code> for the current <code>BVHNode</code>, loop over all
  <code>Primitive</code>s and <code>expand()</code> the <code>BBox</code> with
  each of the <code>Primitive</code>'s bounding boxes. Then, the current
  <code>BVHNode</code> is actually created with the <code>BBox</code>, and its
  <code>start</code> and <code>end</code> are set. If the number of
  <code>Primitive</code>s in the list between <code>start</code> and
  <code>end</code> is less than the <code>max_leaf_size</code>, then this
  current <code>BVHNode</code> should just be returned as a leaf node.
  Otherwise, it is not a leaf node, and we must split it into a left and right
  child. Our heuristic to choose a split point begins with choosing an axis to
  split on, and then choosing a point on that axis with which we partition the
  <code>Primitive</code>s. We chose to split on the median primitive centroid
  along the longest axis of the bounding box as this divides the bounding box
  into two child bounding boxes with a (roughly) equal number of
  <code>Primitive</code>s. Splitting on the longest axes keeps the bounding
  boxes relatively more uniform in size rather than long rectangular prisms. To
  split on the median primitive, we first <code>std::sort()</code> the
  <code>std::vector<Primitive *></code> of primitives with a custom comparator
  function that compares the longest axis coordinate of each primitive's
  bounding box's centroid (its center point in 3D space). Then, we simply
  create a new <code>std::vector<Primitive *>::iterator</code> for the middle
  element of the sorted vector. <code>start</code> and <code>end</code> still
  point to the first and last primitive in the vector, respectively, and so the
  left child node recursive call gets all primitives between <code>start</code>
  to <code>halfway_pt</code>, and the right child node recursive call gets all
  primitives between <code>halfway_pt</code> to <code>end</code>.
</p>

<h3>
  Show images with normal shading for a few large .dae files that you can only render with BVH acceleration.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/part2/CBbunny.png" align="middle" width="400px"/>
        <figcaption>CBbunny.dae</figcaption>
      </td>
      <td>
        <img src="images/part2/CBlucy.png" align="middle" width="400px"/>
        <figcaption>CBlucy.dae</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/part2/beast.png" align="middle" width="400px"/>
        <figcaption>beast.dae</figcaption>
      </td>
      <td>
        <img src="images/part2/maxplanck.png" align="middle" width="400px"/>
        <figcaption>maxplanck.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h3>
  Compare rendering times on a few scenes with moderately complex geometries with and without BVH acceleration. Present your results in a one-paragraph analysis.
</h3>
<p>
  With BVH acceleration and our simple heuristic of splitting on the median
  primitive centroid along the longest axis of the bounding box, we get extreme
  speed up in render times. All times were measured on the Hive to maintain
  consistency and to compare with the times of the staff solution on the Hive
  specified in the spec. As shown in the following images, the render time of
  <code>cow.dae</code> without BVH acceleration is 41.3434 seconds, while its
  render time with BVH acceleration is 0.0918 seconds, which is a 450x speed
  up! The render time of <code>maxplanck.dae</code> without BVH acceleration is
  370.4354 seconds, while its render time with BVH acceleration is 0.1190 seconds,
  producing a 3113x speed up. Finally the render time of <code>CBlucy.dae</code>
  without BVH acceleration is 975.9085 seconds, while its render time without
  BVH acceleration is 0.3816 seconds, giving a 2557x speed up.
</p>
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/part2/cow-time-no-bvh.png" align="middle" width="90%"/>
        <figcaption>cow.dae render time without BVH acceleration</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/part2/cow-time-bvh.png" align="middle" width="90%"/>
        <figcaption>cow.dae render time with BVH acceleration</figcaption>
      </td>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/part2/maxplanck-time-no-bvh.png" align="middle" width="90%"/>
        <figcaption>maxplanck.dae render time without BVH acceleration</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/part2/maxplanck-time-bvh.png" align="middle" width="90%"/>
        <figcaption>maxplanck.dae render time with BVH acceleration</figcaption>
      </td>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/part2/CBlucy-time-no-bvh.png" align="middle" width="90%"/>
        <figcaption>CBlucy.dae render time without BVH acceleration</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/part2/CBlucy-time-bvh.png" align="middle" width="90%"/>
        <figcaption>CBlucy.dae render time with BVH acceleration</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h2 align="middle">Part 3: Direct Illumination (20 Points)</h2>
<!-- Walk through both implementations of the direct lighting function.
Show some images rendered with both implementations of the direct lighting function.
Focus on one particular scene with at least one area light and compare the noise levels in soft shadows when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, not uniform hemisphere sampling.
Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis. -->

<h3>
  Walk through both implementations of the direct lighting function.
</h3>
<p>
    The direct lighting function was implemented in one way by sampling uniformly in a hemisphere or estimating
    only directly from the light sources. Both implementations employed a variation of Monte Carlo estimation. Both
    implementations utilized the <code>hit_p</code> hit point of the camera ray in the scene and
    return a <code>Vector3D</code> of the radiance of the scene at that <code>hit_p</code>. For hemisphere sampling,
    we sample by calling <code>hemisphereSampler->get_sample()</code> for
    <code>scene->lights.size() * ns_area_light</code> times, ultimately generating a sample ray with originating
    from the hit point, in a direction towards a sample in the hemisphere. If this sample ray intersects our <code>bvh</code>,
    we then collect the emission where necessary and calculate the proportion of radiance that gets reflected
    (returned by the <code>f</code> function we implemented). Finally, we multiply these values together and
    normalize by the probability of sampling the above ray. At the very end, we normalize our total sum result by
    the total number of samples taken and return the estimated radiance.
</p>
<p>
    Importance sampling, follows largely the same algorithm, but we iterate over the <code>scene->lights</code>
    of a scene instead, ensuring to sample point lights only once. In both importance and hemisphere sampling
    algorithms, we are using a small <code>EPS_F</code>
    constant to avoid intersecting with the hit point and/or the light itself (task 4), updating the sample ray's
    <code>min_t</code> and <code>max_t</code> accordingly. For the importance sampling algorithm, we check for
    NON-intersections in order to calculate a radiance. Sampling a light source and detecting an intersection
    indicates that the hit point is a shadow, and won't have a radiance. The same multiplication and normalization of
    factors were calculated and summed for the Monte Carlo estimation for this importance sampling algorithm.
</p>

<h3>
  Show some images rendered with both implementations of the direct lighting function.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <!-- Header -->
    <tr align="center">
      <th>
        <b>Uniform Hemisphere Sampling</b>
      </th>
      <th>
        <b>Light Sampling</b>
      </th>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/part3/CBspheres_lambertian_H_64_32.png" align="middle" width="400px"/>
        <figcaption>CBspheres_lambertian.dae</figcaption>
      </td>
      <td>
        <img src="images/part3/CBspheres_lambertian_64_32.png" align="middle" width="400px"/>
        <figcaption>CBspheres_lambertian.dae</figcaption>
      </td>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/part3/CBbunny_H_64_32.png" align="middle" width="400px"/>
        <figcaption>CBbunny.dae</figcaption>
      </td>
      <td>
        <img src="images/part3/CBbunny_64_32.png" align="middle" width="400px"/>
        <figcaption>CBbunny.dae</figcaption>
      </td>
    </tr>
    <br>
    <tr align="center">
      <td>
        <img src="images/part3/dragon_H_64_32.png" align="middle" width="400px"/>
        <figcaption>dragon.dae</figcaption>
      </td>
      <td>
        <img src="images/part3/dragon_64_32.png" align="middle" width="400px"/>
        <figcaption>dragon.dae</figcaption>
      </td>
    </tr>
    <br>
  </table>
</div>
<br>

<h3>
  Focus on one particular scene with at least one area light and compare the noise levels in <b>soft shadows</b> when rendering with 1, 4, 16, and 64 light rays (the -l flag) and with 1 sample per pixel (the -s flag) using light sampling, <b>not</b> uniform hemisphere sampling.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/part3/CBbunny_1_1.png" align="middle" width="400px"/>
        <figcaption>1 Light Ray (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/part3/CBbunny_1_4.png" align="middle" width="400px"/>
        <figcaption>4 Light Rays (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/part3/CBbunny_1_16.png" align="middle" width="400px"/>
        <figcaption>16 Light Rays (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/part3/CBbunny_1_64.png" align="middle" width="400px"/>
        <figcaption>64 Light Rays (CBbunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<p>
    This exhibit changes the amount of light rays each image is rendered with. Evidently, as the number of
    samples per area light increases, the amount of noise decreases. This makes sense as, with more samples of each
    light, we are able to render the color of each pixel with higher accuracy and, therefore, less noise.
</p>
<br>

<h3>
  Compare the results between uniform hemisphere sampling and lighting sampling in a one-paragraph analysis.
</h3>
<p>
    Comparing the results found between <code>CBspheres_lambertian.dae</code> and <code>CBbunny.dae</code>, we see
    that the results obtained from importance sampling each light are far less noisy. In importance sampling,
    the transition between whites (lights) and blacks (shadows) are smoother and the light source itself is
    rendered with cleaner borders rather than a blur/halo, which appears in hemisphere sampling.
</p>
<p>
    An interesting comparison would be the rendering of <code>dragon.dae</code>. With light importance sampling, we see
    the correct render of the dragon with all its details and shadows. However, we see an entirely black image. This
    is because the scene is lit with a point light. With hemisphere sampling, the probability of sampling the point
    light is very low, and so not a lot of images of the dragon are sampled. However, with importance sampling, we
    know exactly where the light is in the scene, and so we are able to render the details of the dragon with this
    algorithm.
</p>
<br>


<h2 align="middle">Part 4: Global Illumination (20 Points)</h2>
<!-- Walk through your implementation of the indirect lighting function.
Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
You will probably want to use the instructional machines for the above renders in order to not burn up your own computer for hours. -->

<h3>
  Walk through your implementation of the indirect lighting function.
</h3>
<p>
  Indirect lighting is lighting resulting from recursive bounces past the first
  bounce. To implement indirect lighting, we first implement
  <code>PathTracer::at_least_one_bounce_radiance()</code>. This function calls
  <code>PathTracer::one_bounce_radiance()</code> and then checks whether it
  should recursively call itself to estimate the higher bounces. We sample the
  current intersection's <code>BSDF</code> using <code>BSDF::sample_f()</code>.
  This function returns the BSDF reflectance $f_r$ and writes out a sampled
  direction $w_i$ and its probability density function evaluated at $w_i$.
  We create a sample ray with direction $w_i$ transformed to world space and
  originating from <code>hit_p</code>, the original intersection point. The
  sample ray's <code>min_t</code> value should be <code>EPS_F</code> to avoid
  intersection at the origin with the current object in the scene. Its
  <code>depth</code> value should be the original ray's <code>depth - 1</code>
  to decrement the remaining maximum depth/number of recursive calls. Then,
  we intersect the sample ray with the <code>BVH</code> of the scene to find
  the next intersection point. If the sample ray doesn't intersect anything
  in the scene, we simply return <code>L_out</code>, which is just the value
  of <code>one_bounce_radiance()</code> at this point. Then we check if the
  current ray has more depth left. If so, we use Russian Roulette with
  <code>continuation_prob = 1.0 - termination_prob = 1.0 - 0.35</code> to
  determine whether we make a recursive call and sum the calculated reflection
  equation value into <code>L_out</code>. Finally, the function returns
  <code>L_out</code> once the recursion terminates.
</p>
<br>

<h3>
  Show some images rendered with global (direct and indirect) illumination. Use 1024 samples per pixel.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/part4/spheres.png" align="middle" width="400px"/>
        <figcaption>CBspheres_lambertian.dae</figcaption>
      </td>
      <td>
        <img src="images/part4/dragon.png" align="middle" width="400px"/>
        <figcaption>dragon.dae</figcaption>
      </td>
    </tr>
  </table>
  <table>
    <tr align="center">
      <td>
        <img src="images/part4/wall-e.png" align="middle" width="400px"/>
        <figcaption>wall-e.dae</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>

<h3>
  Pick one scene and compare rendered views first with only direct illumination, then only indirect illumination. Use 1024 samples per pixel. (You will have to edit PathTracer::at_least_one_bounce_radiance(...) in your code to generate these views.)
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/part4/spheres_only_direct_illumination.png" align="middle" width="400px"/>
        <figcaption>Only direct illumination (CBspheres_lambertian.dae)</figcaption>
      </td>
      <td>
        <img src="images/part4/spheres_only_indirect_illumination.png" align="middle" width="400px"/>
        <figcaption>Only indirect illumination (CBspheres_lambertian.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
  On the left, we see <code>CBspheres_lambertian.dae</code> rendered with only
  direct illumination. This was done by only summing the
  <code>zero_bounce_radiance</code> and <code>one_bounce_radiance</code>, so no
  higher recursive bounces are rendered. The result is the same as what we see
  in part 3 where there is no color reflections/bleeding onto the spheres. On
  the right, we see <code>CBspheres_lambertian.dae</code> rendered with only
  indirect illumination. This was done by only summing
  <code>zero_bounce_radiance</code> and the higher recursive bounces after the
  <code>one_bounce_radiance</code>. Implementation-wise, we simply subtract out
  <code>one_bounce_radiance</code> from the normal sum of
  <code>zero_bounce_radiance + at_least_one_bounce_radiance</code>. The result
  is that we still see the area light source from the
  <code>zero_bounce_radiance</code>, but now we don't see the dark shadows from
  the <code>one_bounce_radiance</code> direct illumination. We also see the
  reflected colors from the walls onto the spheres and elsewhere from the
  higher recursive bounces.
</p>
<br>

<h3>
  For CBbunny.dae, compare rendered views with max_ray_depth set to 0, 1, 2, 3, and 100 (the -m flag). Use 1024 samples per pixel.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/part4/CBbunny_m_0.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 0 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/part4/CBbunny_m_1.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 1 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/part4/CBbunny_m_2.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 2 (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/part4/CBbunny_m_3.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 3 (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/part4/CBbunny_m_100.png" align="middle" width="400px"/>
        <figcaption>max_ray_depth = 100 (CBbunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
  These renders were done with no Russian Roulette termination so that the
  number of recursive bounces would match the <code>max_ray_depth</code>. As
  the <code>max_ray_depth</code> increases, the more recursive bounces are
  performed and summed, and the brighter the reflected light becomes. However,
  as we know there is decay in the reflected light as the number of bounces
  increases, so at 100 bounces, the image is not that much brighter visually
  than the image with 3 bounces.
</p>
<br>

<h3>
  Pick one scene and compare rendered views with various sample-per-pixel rates, including at least 1, 2, 4, 8, 16, 64, and 1024. Use 4 light rays.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/part4/CBbunny_s_1_l_4.png" align="middle" width="400px"/>
        <figcaption>1 sample per pixel (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/part4/CBbunny_s_2_l_4.png" align="middle" width="400px"/>
        <figcaption>2 samples per pixel (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/part4/CBbunny_s_4_l_4.png" align="middle" width="400px"/>
        <figcaption>4 samples per pixel (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/part4/CBbunny_s_8_l_4.png" align="middle" width="400px"/>
        <figcaption>8 samples per pixel (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/part4/CBbunny_s_16_l_4.png" align="middle" width="400px"/>
        <figcaption>16 samples per pixel (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/part4/CBbunny_s_64_l_4.png" align="middle" width="400px"/>
        <figcaption>64 samples per pixel (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/part4/CBbunny_s_1024_l_4.png" align="middle" width="400px"/>
        <figcaption>1024 samples per pixel (CBbunny.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>
<p>
  These renders were done with Russian Roulette and <code>max_ray_depth</code>
  termination. As the number of samples per pixel increases, the amount of
  noise decreases. At 1024 samples per pixel, the noise is much less
  noticeable. The number of samples per pixel is exactly the number of camera
  rays shot into the scene per image pixel. Each camera ray intersects the
  scene at some point, and our whole ray tracing and recursive illumination
  calculation occurs. The more times we do this per pixel, the more
  terms/values we calculate in our illumination estimate, reducing the amount
  of noise we see in the final renders.
</p>
<br>


<h2 align="middle">Part 5: Adaptive Sampling (20 Points)</h2>
<!-- Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
Pick one scene and render it with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth. -->

<h3>
  Explain adaptive sampling. Walk through your implementation of the adaptive sampling.
</h3>
<p>
  Path tracing with Monte Carlo estimation is powerful, but it produces a large
  amount of noise. We can reduce noise by increasing the number of samples per
  pixel, but this becomes expensive quickly, and we actually don't need to do
  this uniformly for all pixels. Adaptive sampling is a method we can use to
  move away from using a fixed (high) number of samples per pixel and instead
  potentially use less for pixels that converge faster with low sampling rates.
  This efficiently concentrates samples for pixels containing more difficult
  parts of the image.
</p>
<p>
  To implement, adaptive sampling, we modify
  <code>PathTracer::raytrace_pixel()</code>. Within the sampling loop, we
  calculate the illuminance from the estimated radiance of the generated camera
  ray as $x_k$. We then calculate the sums (so far) $s_1 = \sum_{k=1}^{i} x_k$
  and $s_2 = \sum_{k=1}^{i} x_k^2$. We sum the sampled radiance into the
  running sum of radiance for Monte Carlo estimation. Then we check if the
  current number of samples <code>i</code> is a multiple of
  <code>samplesPerBatch</code>. If so, we calculate the mean $\mu = \frac {s_1}
  {i}$ and variance $\sigma^2 = \frac {1} {i - 1} \cdot (s_2 - \frac {s_1^2}
  {i})$. If $I <= maxTolerance \cdot \mu$, where $I = 1.96 \cdot \frac {\sigma}
  {sqrt(i)}$, then we can confidently assume that the pixel has converged and
  we break out of the for loop to stop sampling further. Finally, instead of
  dividing the sum of radiance by the maximum number of samples
  <code>num_samples</code>, we divide by the actual number of samples
  <code>i</code> to correctly calculate the Monte Carlo estimate. We also set
  <code>sampleCountBuffer[x + y * sampleBuffer.w] = i;</code> for the same
  reason.
</p>
<br>

<h3>
  Pick two scenes and render them with at least 2048 samples per pixel. Show a good sampling rate image with clearly visible differences in sampling rate over various regions and pixels. Include both your sample rate image, which shows your how your adaptive sampling changes depending on which part of the image you are rendering, and your noise-free rendered result. Use 1 sample per light and at least 5 for max ray depth.
</h3>
<!-- Example of including multiple figures -->
<div align="middle">
  <table style="width:100%">
    <tr align="center">
      <td>
        <img src="images/part5/bunny_adaptive_sampling.png" align="middle" width="400px"/>
        <figcaption>Rendered image (CBbunny.dae)</figcaption>
      </td>
      <td>
        <img src="images/part5/bunny_adaptive_sampling_rate.png" align="middle" width="400px"/>
        <figcaption>Sample rate image (CBbunny.dae)</figcaption>
      </td>
    </tr>
    <tr align="center">
      <td>
        <img src="images/part5/spheres_adaptive_sampling.png" align="middle" width="400px"/>
        <figcaption>Rendered image (CBspheres_lambertian.dae)</figcaption>
      </td>
      <td>
        <img src="images/part5/spheres_adaptive_sampling_rate.png" align="middle" width="400px"/>
        <figcaption>Sample rate image (CBspheres_lambertian.dae)</figcaption>
      </td>
    </tr>
  </table>
</div>
<br>


</body>
</html>
